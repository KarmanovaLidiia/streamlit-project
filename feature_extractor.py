import pandas as pd
import numpy as np
import re
from typing import Dict, List, Tuple, Optional
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

warnings.filterwarnings('ignore')


class RussianFeatureExtractor:
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ä–∞–±–æ—Ç–∞—é—â–∏–º composite_quality_score"""

    def __init__(self, use_heavy_models: bool = False):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        self.use_heavy_models = use_heavy_models
        self.sbert_model = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_models()

        # –°–ø–∏—Å–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        self.greeting_words = ['–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '–ø—Ä–∏–≤–µ—Ç', '–¥–æ–±—Ä—ã–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä–æ–µ', '–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é']
        self.question_words = ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–º–æ–∂–Ω–æ', '—Å–∫–æ–ª—å–∫–æ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è']
        self.descriptive_words = ['–≤–∏–∂—É', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–¥–µ–ª–∞–µ—Ç', '–æ–¥–µ—Ç', '—Å—Ç–æ–∏—Ç', '—Å–∏–¥–∏—Ç']
        self.connector_words = ['–ø–æ—Ç–æ–º—É —á—Ç–æ', '–ø–æ—ç—Ç–æ–º—É', '—Ç–∞–∫ –∫–∞–∫', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–∫—Ä–æ–º–µ —Ç–æ–≥–æ']
        self.emotional_words = ['–∫—Ä–∞—Å–∏–≤–æ', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–Ω—Ä–∞–≤–∏—Ç—Å—è']
        self.spatial_words = ['—Å–ª–µ–≤–∞', '—Å–ø—Ä–∞–≤–∞', '–≤–≤–µ—Ä—Ö—É', '–≤–Ω–∏–∑—É', '—Ä—è–¥–æ–º', '–æ–∫–æ–ª–æ']

        print("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        if self.use_heavy_models:
            print("‚ÑπÔ∏è –¢—è–∂–µ–ª—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏")
        print("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫–∏–µ –º–µ—Ç–æ–¥—ã (TF-IDF)")

    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""
        text = str(text)
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å.,!?;:()-]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_basic_features(self, text: str) -> Dict[str, float]:
        """–ë–∞–∑–æ–≤—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        text_clean = self.clean_text(text)

        if not text_clean:
            return {
                'text_length': 0, 'word_count': 0, 'sentence_count': 0,
                'avg_word_length': 0, 'lexical_diversity': 0,
                'has_questions': 0, 'has_exclamations': 0
            }

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text_clean.lower())
        sentences = [s.strip() for s in re.split(r'[.!?]+', text_clean) if s.strip()]

        word_count = len(words)
        text_length = len(text_clean)
        sentence_count = len(sentences)

        features = {
            'text_length': text_length,
            'word_count': word_count,
            'sentence_count': sentence_count,
            'avg_word_length': sum(len(w) for w in words) / max(word_count, 1),
            'lexical_diversity': len(set(words)) / max(word_count, 1),
            'has_questions': int('?' in text_clean),
            'has_exclamations': int('!' in text_clean),
        }

        return features

    def extract_semantic_features(self, question: str, answer: str) -> Dict[str, float]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        question_clean = self.clean_text(question)
        answer_clean = self.clean_text(answer)

        features = {
            'keyword_overlap': 0.0,
            'response_relevance': 0.0
        }

        if not answer_clean or not question_clean:
            return features

        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            question_words = set(re.findall(r'\b[–∞-—è—ë]+\b', question_clean.lower()))
            answer_words = set(re.findall(r'\b[–∞-—è—ë]+\b', answer_clean.lower()))

            if question_words:
                common_words = question_words.intersection(answer_words)
                features['keyword_overlap'] = len(common_words) / max(len(question_words), 1)
                features['response_relevance'] = min(1.0, len(answer_words) / max(len(question_words), 1))

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")

        return features

    def extract_grammar_features(self, text: str) -> Dict[str, float]:
        """–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        text_clean = self.clean_text(text)

        features = {
            'grammar_quality': 0.5,  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            'has_punctuation': 0.0,
            'sentence_completeness': 0.0
        }

        if not text_clean:
            return features

        sentences = [s.strip() for s in re.split(r'[.!?]+', text_clean) if s.strip()]
        words = text_clean.split()

        if sentences:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
            features['has_punctuation'] = 1.0 if any(mark in text_clean for mark in '.!?') else 0.0

            # –ü–æ–ª–Ω–æ—Ç–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            complete_sentences = sum(1 for s in sentences if len(s.split()) >= 3)
            features['sentence_completeness'] = complete_sentences / max(len(sentences), 1)

            # –£–ª—É—á—à–µ–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            grammar_score = 0.0
            grammar_score += features['has_punctuation'] * 0.3
            grammar_score += features['sentence_completeness'] * 0.4

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
            if len(words) > 5:
                avg_sentence_len = len(words) / len(sentences)
                if 5 <= avg_sentence_len <= 20:
                    grammar_score += 0.2
                elif avg_sentence_len > 20:
                    grammar_score += 0.1

            features['grammar_quality'] = min(1.0, grammar_score)

        return features

    def extract_style_features(self, text: str) -> Dict[str, float]:
        """–°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        text_clean = self.clean_text(text).lower()

        features = {
            'has_greeting': 0.0,
            'has_description': 0.0,
            'has_connectors': 0.0,
            'has_emotional_words': 0.0,
            'style_score': 0.0
        }

        if not text_clean:
            return features

        # –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã
        features.update({
            'has_greeting': float(any(greet in text_clean for greet in self.greeting_words)),
            'has_description': float(any(desc in text_clean for desc in self.descriptive_words)),
            'has_connectors': float(any(conn in text_clean for conn in self.connector_words)),
            'has_emotional_words': float(any(emot in text_clean for emot in self.emotional_words)),
        })

        # –û—Ü–µ–Ω–∫–∞ —Å—Ç–∏–ª—è
        style_indicators = sum([
            features['has_greeting'],
            features['has_connectors'],
            features['has_emotional_words']
        ])
        features['style_score'] = min(1.0, style_indicators / 3)

        return features

    def extract_quality_features(self, text: str, question_type: int) -> Dict[str, float]:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
        text_clean = self.clean_text(text)
        words = text_clean.split()
        word_count = len(words)

        features = {
            'answer_length_sufficiency': min(1.0, word_count / 30),  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞
            'content_richness': 0.0,
            'engagement_level': 0.0
        }

        if not text_clean:
            return features

        # –ë–æ–≥–∞—Ç—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ + –¥–ª–∏–Ω–∞)
        lexical_diversity = len(set(words)) / max(word_count, 1)
        features['content_richness'] = min(1.0, (lexical_diversity + features['answer_length_sufficiency']) / 2)

        # –£—Ä–æ–≤–µ–Ω—å –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
        engagement = 0.0
        engagement += features['answer_length_sufficiency'] * 0.4
        engagement += lexical_diversity * 0.3
        engagement += (1.0 if '?' in text_clean else 0.0) * 0.3
        features['engagement_level'] = engagement

        return features

    def extract_all_features(self, row: pd.Series) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        try:
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            question = row.get('–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', row.get('–í–æ–ø—Ä–æ—Å', ''))
            answer = row.get('–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', row.get('–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç', row.get('–û—Ç–≤–µ—Ç', '')))
            question_type = row.get('‚Ññ –≤–æ–ø—Ä–æ—Å–∞', row.get('–¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞', 1))

            try:
                question_type = int(question_type)
            except:
                question_type = 1

            features = {}

            # 1. –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–∞–¥–µ–∂–Ω—ã–µ)
            basic_features = self.extract_basic_features(answer)
            features.update(basic_features)

            # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            semantic_features = self.extract_semantic_features(question, answer)
            features.update(semantic_features)

            # 3. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            grammar_features = self.extract_grammar_features(answer)
            features.update(grammar_features)

            # 4. –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            style_features = self.extract_style_features(answer)
            features.update(style_features)

            # 5. –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_features = self.extract_quality_features(answer, question_type)
            features.update(quality_features)

            # 6. –¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞
            features['question_type'] = float(question_type)

            # 7. –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å
            features['composite_quality_score'] = self._calculate_quality_score(features)

            return features

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            return self._get_fallback_features()

    def _calculate_quality_score(self, features: Dict[str, float]) -> float:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —Ä–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""

        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        weights = {
            # –°–µ–º–∞–Ω—Ç–∏–∫–∞ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (35%)
            'keyword_overlap': 0.20,
            'response_relevance': 0.15,

            # –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (25%)
            'grammar_quality': 0.15,
            'sentence_completeness': 0.10,

            # –°—Ç–∏–ª—å –∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å (25%)
            'style_score': 0.10,
            'engagement_level': 0.15,

            # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (15%)
            'content_richness': 0.15
        }

        total_score = 0.0
        total_weight = 0.0

        for feature, weight in weights.items():
            if feature in features:
                value = features[feature]
                total_score += value * weight
                total_weight += weight

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–ª—É—á–∞–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if total_weight > 0:
            final_score = total_score / total_weight
        else:
            final_score = 0.5  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

        return min(1.0, max(0.0, final_score))

    def _get_fallback_features(self) -> Dict[str, float]:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {
            'text_length': 0, 'word_count': 0, 'sentence_count': 0,
            'avg_word_length': 0, 'lexical_diversity': 0,
            'has_questions': 0, 'has_exclamations': 0,
            'keyword_overlap': 0, 'response_relevance': 0,
            'grammar_quality': 0.5, 'has_punctuation': 0, 'sentence_completeness': 0,
            'has_greeting': 0, 'has_description': 0, 'has_connectors': 0,
            'has_emotional_words': 0, 'style_score': 0,
            'answer_length_sufficiency': 0, 'content_richness': 0, 'engagement_level': 0,
            'question_type': 1, 'composite_quality_score': 0.5
        }

    def extract_features_for_dataframe(self, df: pd.DataFrame, sample_size: int = None) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞"""
        if sample_size and sample_size < len(df):
            df = df.sample(sample_size, random_state=42)
            print(f"–í–∑—è—Ç–∞ –≤—ã–±–æ—Ä–∫–∞: {len(df)} —Å—Ç—Ä–æ–∫")

        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(df)} —Å—Ç—Ä–æ–∫...")
        features_list = []
        successful = 0

        for idx, row in df.iterrows():
            if idx % 50 == 0 and idx > 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx}/{len(df)} —Å—Ç—Ä–æ–∫...")

            try:
                features = self.extract_all_features(row)
                features['original_index'] = idx
                features_list.append(features)
                successful += 1
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {idx}: {e}")
                continue

        if features_list:
            features_df = pd.DataFrame(features_list)
            features_df.set_index('original_index', inplace=True)

            success_rate = successful / len(df)
            print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –£—Å–ø–µ—à–Ω–æ: {successful}/{len(df)} ({success_rate:.1%})")

            return features_df
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏")
            return pd.DataFrame()


# –ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def extract_quick_features(text: str) -> Dict[str, float]:
    extractor = RussianFeatureExtractor()
    return extractor.extract_basic_features(text)


if __name__ == "__main__":
    # –¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
    extractor = RussianFeatureExtractor()
    test_data = {
        '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞': ['–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ –≤–∞—à–µ–º –≥–æ—Ä–æ–¥–µ'],
        '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞': ['–ü—Ä–∏–≤–µ—Ç! –Ø –∂–∏–≤—É –≤ –ú–æ—Å–∫–≤–µ. –≠—Ç–æ –±–æ–ª—å—à–æ–π –∏ –∫—Ä–∞—Å–∏–≤—ã–π –≥–æ—Ä–æ–¥ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∫–æ–≤ –∏ –º—É–∑–µ–µ–≤.'],
        '‚Ññ –≤–æ–ø—Ä–æ—Å–∞': [1]
    }
    test_df = pd.DataFrame(test_data)
    features = extractor.extract_all_features(test_df.iloc[0])

    print("üéØ –¢–ï–°–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –í–ï–†–°–ò–ò:")
    print(f"–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å: {features['composite_quality_score']:.3f}")
    print(f"–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {features['grammar_quality']:.3f}")
    print(f"–°—Ç–∏–ª–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å: {features['style_score']:.3f}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {features['word_count']}")