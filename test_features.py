import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from feature_extractor import RussianFeatureExtractor
import os
import sys
import subprocess
import traceback
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re


def check_environment():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    print("=== –ü–†–û–í–ï–†–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø ===")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–∫–µ—Ç–æ–≤
    packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'torch']
    for package in packages:
        try:
            __import__(package)
            print(f"‚úÖ {package}")
        except ImportError as e:
            print(f"‚ùå {package}: {e}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Java
    try:
        subprocess.run(['java', '-version'], capture_output=True, check=True)
        print("‚úÖ Java —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    except:
        print("‚ùå Java –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ - –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å")


def load_and_analyze_dataset():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
    print("\n=== –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• ===")

    try:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏
        for filename in ['small.csv', 'dataset.csv', 'train.csv']:
            if os.path.exists(filename):
                print(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {filename}")

                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                for delimiter in [';', ',', '\t']:
                    try:
                        df = pd.read_csv(filename, encoding='utf-8', delimiter=delimiter)
                        if len(df.columns) > 1:  # –£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º '{delimiter}'")
                            break
                    except:
                        continue
                else:
                    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å")
                    return None

                print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
                print(f"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")

                # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
                print("\n--- –°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–• ---")
                for col in df.columns:
                    print(f"{col}: {df[col].dtype}, –ø—Ä–æ–ø—É—Å–∫–æ–≤: {df[col].isnull().sum()}")

                    if df[col].dtype == 'object':
                        sample = df[col].iloc[0] if not df[col].isnull().all() else "N/A"
                        print(f"  –ü—Ä–∏–º–µ—Ä: {str(sample)[:100]}...")

                # –ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                question_cols = [col for col in df.columns if '–≤–æ–ø—Ä–æ—Å' in col.lower()]
                transcript_cols = [col for col in df.columns if '—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç' in col.lower()]
                score_cols = [col for col in df.columns if '–æ—Ü–µ–Ω–∫' in col.lower() or '–±–∞–ª–ª' in col.lower()]

                print(f"\n--- –í–´–Ø–í–õ–ï–ù–ù–´–ï –ö–û–õ–û–ù–ö–ò ---")
                print(f"–í–æ–ø—Ä–æ—Å—ã: {question_cols}")
                print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã: {transcript_cols}")
                print(f"–û—Ü–µ–Ω–∫–∏: {score_cols}")

                return df

        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏")
        return None

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None


def test_alternative_features(texts):
    """–¢–µ—Å—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n=== –¢–ï–°–¢ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í ===")

    features_list = []

    for i, text in enumerate(texts):
        if pd.isna(text):
            features_list.append({})
            continue

        text_str = str(text)
        features = {}

        # –ë–∞–∑–æ–≤—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        features['text_length'] = len(text_str)

        words = re.findall(r'\b[–∞-—è—ëa-z]+\b', text_str.lower())
        features['word_count'] = len(words)

        sentences = re.split(r'[.!?]+', text_str)
        features['sentence_count'] = len([s for s in sentences if len(s.strip()) > 10])

        features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0
        features['lexical_diversity'] = len(set(words)) / len(words) if words else 0

        # –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        features['has_questions'] = int('?' in text_str)
        features['has_exclamations'] = int('!' in text_str)
        features['has_ellipsis'] = int('...' in text_str)

        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        long_words = [w for w in words if len(w) > 6]
        features['long_word_ratio'] = len(long_words) / len(words) if words else 0

        features_list.append(features)

        if i < 3:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 —Ç–µ–∫—Å—Ç–æ–≤
            print(f"–ü—Ä–∏–º–µ—Ä {i + 1}: {text_str[:80]}...")
            for k, v in features.items():
                print(f"  {k}: {v:.3f}")

    return pd.DataFrame(features_list)


def enhanced_feature_extraction(df):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    print("\n=== –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í ===")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º–∏
    transcript_cols = [col for col in df.columns if '—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç' in col.lower()]
    if not transcript_cols:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º–∏")
        return pd.DataFrame()

    transcript_col = transcript_cols[0]
    texts = df[transcript_col].fillna('')

    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts)} —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤...")

    try:
        # –ü—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
        print("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RussianFeatureExtractor...")
        extractor = RussianFeatureExtractor()
        features_df = extractor.extract_features_for_dataframe(df)

        if not features_df.empty:
            print("‚úÖ RussianFeatureExtractor —É—Å–ø–µ—à–Ω–æ –æ—Ç—Ä–∞–±–æ—Ç–∞–ª")
            return features_df
        else:
            print("‚ùå RussianFeatureExtractor –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π DataFrame")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ RussianFeatureExtractor: {e}")
        print("üîÑ –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥...")

    # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    features_df = test_alternative_features(texts)

    return features_df


def analyze_correlations_with_scores(features_df, original_df):
    """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏"""
    print("\n=== –ê–ù–ê–õ–ò–ó –ö–û–†–†–ï–õ–Ø–¶–ò–ô ===")

    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –æ—Ü–µ–Ω–∫–∞–º–∏
    score_cols = [col for col in original_df.columns if '–æ—Ü–µ–Ω–∫' in col.lower() or '–±–∞–ª–ª' in col.lower()]
    if not score_cols:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ —Å –æ—Ü–µ–Ω–∫–∞–º–∏")
        return

    score_col = score_cols[0]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
    analysis_df = features_df.copy()
    analysis_df['real_score'] = original_df[score_col].values

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
    analysis_clean = analysis_df.dropna()

    if len(analysis_clean) < 2:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π")
        return

    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    correlations = analysis_clean.corr()['real_score'].sort_values(key=abs, ascending=False)

    print("\n–¢–æ–ø-15 –Ω–∞–∏–±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print("-" * 50)

    for feature, corr in correlations.items():
        if feature != 'real_score':
            direction = "+" if corr > 0 else "-"
            significance = "***" if abs(corr) > 0.3 else "**" if abs(corr) > 0.2 else "*" if abs(corr) > 0.1 else ""
            print(f"  {direction} {feature}: {corr:+.3f} {significance}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    top_features = correlations.head(6).index.tolist()
    if 'real_score' in top_features:
        top_features.remove('real_score')

    if top_features:
        plt.figure(figsize=(12, 8))

        for i, feature in enumerate(top_features[:5]):
            plt.subplot(2, 3, i + 1)
            plt.scatter(analysis_clean[feature], analysis_clean['real_score'], alpha=0.6)
            plt.xlabel(feature)
            plt.ylabel('Real Score')
            plt.title(f'r = {correlations[feature]:.3f}')

        plt.tight_layout()
        plt.show()

    return analysis_clean


def save_detailed_report(features_df, original_df, analysis_df):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print("\n=== –°–û–•–†–ê–ù–ï–ù–ò–ï –û–¢–ß–ï–¢–ê ===")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features_df.to_csv('extracted_features_enhanced.csv', encoding='utf-8')
    print("‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ extracted_features_enhanced.csv")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    with open('features_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write("–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ü–†–ò–ó–ù–ê–ö–û–í\n")
        f.write("=" * 50 + "\n\n")

        f.write(f"–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
        f.write(f"- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(features_df)}/{len(original_df)}\n")
        f.write(f"- –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features_df.columns)}\n")
        f.write(f"- –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {features_df.notna().mean().mean():.1%}\n\n")

        f.write("–°–ü–ò–°–û–ö –ü–†–ò–ó–ù–ê–ö–û–í:\n")
        for col in features_df.columns:
            f.write(f"\n{col}:\n")
            f.write(f"  –¢–∏–ø: {features_df[col].dtype}\n")
            f.write(f"  –ù–µ-NULL: {features_df[col].notna().sum()}\n")
            f.write(f"  –°—Ä–µ–¥–Ω–µ–µ: {features_df[col].mean():.3f}\n")
            f.write(f"  Std: {features_df[col].std():.3f}\n")

            if analysis_df is not None and 'real_score' in analysis_df.columns:
                corr = analysis_df.corr()['real_score'].get(col, 0)
                f.write(f"  –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –æ—Ü–µ–Ω–∫–æ–π: {corr:+.3f}\n")

        if analysis_df is not None:
            f.write("\n–ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó:\n")
            correlations = analysis_df.corr()['real_score'].sort_values(key=abs, ascending=False)
            for feature, corr in correlations.items():
                if feature != 'real_score' and abs(corr) > 0.1:
                    f.write(f"  {feature}: {corr:+.3f}\n")

    print("‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ features_analysis_report.txt")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    check_environment()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = load_and_analyze_dataset()
    if df is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features_df = enhanced_feature_extraction(df)

    if features_df.empty:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏")
        return

    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    analysis_df = analyze_correlations_with_scores(features_df, df)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_detailed_report(features_df, df, analysis_df)

    print("\n" + "=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 60)


if __name__ == "__main__":
    main()