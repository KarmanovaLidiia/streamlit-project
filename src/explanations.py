from __future__ import annotations
import pandas as pd
import numpy as np
from typing import List


def add_score_explanations(feats: pd.DataFrame, pred_scores: np.ndarray) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏—á–µ–π
    """
    out = feats.copy()
    explanations = []

    for idx, row in feats.iterrows():
        q_num = row['question_number']
        pred_score = pred_scores[idx]
        explanation_parts = []

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if row.get('ans_len_words', 0) < 10:
            explanation_parts.append("üî¥ –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç")
        elif row.get('ans_len_words', 0) > 50:
            explanation_parts.append("üü¢ –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç")
        else:
            explanation_parts.append("üü° –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞")

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        semantic_sim = row.get('semantic_sim', 0)
        if semantic_sim > 0.7:
            explanation_parts.append("‚úÖ –í—ã—Å–æ–∫–æ–µ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ")
        elif semantic_sim > 0.4:
            explanation_parts.append("‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ")
        else:
            explanation_parts.append("‚ùå –ù–∏–∑–∫–æ–µ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ")

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞
        if row.get('ans_n_sents', 0) >= 3:
            explanation_parts.append("üìä –•–æ—Ä–æ—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞")
        else:
            explanation_parts.append("üìâ –ú–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")

        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        if q_num == 4:
            # –î–ª—è –≤–æ–ø—Ä–æ—Å–∞ 4 - –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏
            if row.get('q4_has_intro', 0) == 1:
                explanation_parts.append("üé® –ï—Å—Ç—å –≤—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏")
            if row.get('q4_has_personal', 0) == 1:
                explanation_parts.append("üë§ –ï—Å—Ç—å –ª–∏—á–Ω—ã–π –æ–ø—ã—Ç")
            if row.get('q4_coverage_ratio', 0) > 0.6:
                explanation_parts.append("üìã –•–æ—Ä–æ—à–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å–æ–≤")

        elif q_num in [1, 3]:
            # –î–ª—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            if row.get('ans_ttr', 0) > 0.6:
                explanation_parts.append("üí¨ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞")

        elif q_num == 2:
            # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∂–∏–ª—å–µ
            if row.get('ans_len_words', 0) > 30:
                explanation_parts.append("üè† –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if pred_score >= (2.0 if q_num in [2, 4] else 1.0) * 0.8:
            explanation_parts.append("‚≠ê –í—ã—Å–æ–∫–∏–π –±–∞–ª–ª")
        elif pred_score >= (2.0 if q_num in [2, 4] else 1.0) * 0.5:
            explanation_parts.append("‚ö° –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª")
        else:
            explanation_parts.append("üí§ –ù–∏–∑–∫–∏–π –±–∞–ª–ª")

        explanations.append(" | ".join(explanation_parts))

    out['score_explanation'] = explanations
    return out