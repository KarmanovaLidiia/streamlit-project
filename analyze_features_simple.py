import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


def analyze_extracted_features():
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_df = pd.read_csv('real_data_features.csv', index_col=0)
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª real_data_features.csv –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ test_real_data.py")
        return

    print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ò–ó–í–õ–ï–ß–ï–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    print("=" * 50)

    print(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features_df.columns)}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(features_df)}")

    # –ê–Ω–∞–ª–∏–∑ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏
    null_analysis = features_df.isnull().sum()
    null_features = null_analysis[null_analysis > 0]

    if len(null_features) > 0:
        print(f"\n‚ùå –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:")
        for feature, null_count in null_features.items():
            print(f"   {feature}: {null_count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({null_count / len(features_df):.1%})")
    else:
        print(f"\n‚úÖ –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–ø–æ–ª–Ω–µ–Ω—ã!")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    numeric_features = features_df.select_dtypes(include=[np.number])

    print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í:")
    stats_summary = numeric_features.agg(['mean', 'std', 'min', 'max']).T
    stats_summary['cv'] = stats_summary['std'] / stats_summary['mean']  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 —Å–∞–º—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    informative_features = stats_summary[stats_summary['std'] > 0].sort_values('cv', ascending=False)

    print(f"\nüéØ –¢–û–ü-15 —Å–∞–º—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏):")
    for feature, row in informative_features.head(15).iterrows():
        print(f"   {feature:25} mean={row['mean']:6.2f} std={row['std']:6.2f} cv={row['cv']:.2f}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    key_features = ['text_length', 'word_count', 'lexical_diversity', 'composite_quality_score']
    available_features = [f for f in key_features if f in numeric_features.columns]

    if available_features:
        plt.figure(figsize=(15, 10))

        for i, feature in enumerate(available_features, 1):
            plt.subplot(2, 2, i)
            plt.hist(numeric_features[feature].dropna(), bins=20, alpha=0.7, edgecolor='black')
            plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {feature}')
            plt.xlabel(feature)
            plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')

        plt.tight_layout()
        plt.savefig('features_distribution.png', dpi=150, bbox_inches='tight')
        plt.show()
        print(f"\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ features_distribution.png")

    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è
    if 'composite_quality_score' in numeric_features.columns:
        print(f"\nüéØ –ê–ù–ê–õ–ò–ó –ö–û–ú–ü–û–ó–ò–¢–ù–û–ì–û –ü–û–ö–ê–ó–ê–¢–ï–õ–Ø –ö–ê–ß–ï–°–¢–í–ê:")
        quality_scores = numeric_features['composite_quality_score']
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {quality_scores.mean():.3f}")
        print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {quality_scores.std():.3f}")
        print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: [{quality_scores.min():.3f}, {quality_scores.max():.3f}]")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º
        quantiles = quality_scores.quantile([0.25, 0.5, 0.75])
        print(f"   –ö–≤–∞–Ω—Ç–∏–ª–∏: 25%={quantiles[0.25]:.3f}, 50%={quantiles[0.5]:.3f}, 75%={quantiles[0.75]:.3f}")

        # –ê–Ω–∞–ª–∏–∑ —á—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ
        print(f"\nüîç –ö–û–†–†–ï–õ–Ø–¶–ò–Ø –° –ö–û–ú–ü–û–ó–ò–¢–ù–´–ú –ü–û–ö–ê–ó–ê–¢–ï–õ–ï–ú:")
        correlations = numeric_features.corr()['composite_quality_score'].abs().sort_values(ascending=False)

        for feature, corr in correlations.head(10).items():
            if feature != 'composite_quality_score':
                actual_corr = numeric_features.corr()['composite_quality_score'][feature]
                direction = "‚Üë" if actual_corr > 0 else "‚Üì"
                print(f"   {direction} {feature:25} {actual_corr:+.3f}")


def check_feature_correlations_with_target():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ –µ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏)"""

    try:
        features_df = pd.read_csv('real_data_features.csv', index_col=0)
    except FileNotFoundError:
        return

    # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    score_columns = [col for col in features_df.columns if
                     'score' in col.lower() or '–æ—Ü–µ–Ω–∫' in col.lower() or '–±–∞–ª–ª' in col.lower()]

    if score_columns:
        target_col = score_columns[0]
        print(f"\nüéØ –ö–û–†–†–ï–õ–Ø–¶–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í –° {target_col}:")
        print("-" * 50)

        correlations = features_df.corr()[target_col].abs().sort_values(ascending=False)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        top_correlated = correlations.head(11)  # +1 –ø–æ—Ç–æ–º—É —á—Ç–æ target —Å–∞–º —Å —Å–æ–±–æ–π

        print(f"   {'–ü–†–ò–ó–ù–ê–ö':<25} {'–ö–û–†–†–ï–õ–Ø–¶–ò–Ø':<10} {'–ó–ù–ê–ß–ò–ú–û–°–¢–¨'}")
        print(f"   {'-' * 25} {'-' * 10} {'-' * 10}")

        for feature, corr in top_correlated.items():
            if feature != target_col:
                actual_corr = features_df.corr()[target_col][feature]
                direction = "‚Üë" if actual_corr > 0 else "‚Üì"
                significance = "***" if abs(actual_corr) > 0.3 else "**" if abs(actual_corr) > 0.2 else "*" if abs(
                    actual_corr) > 0.1 else ""
                print(f"   {direction} {feature:<23} {actual_corr:+.3f}    {significance}")
    else:
        print(f"\n‚ÑπÔ∏è –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–æ—Ü–µ–Ω–∫–∏) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")


def analyze_feature_categories():
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""

    try:
        features_df = pd.read_csv('real_data_features.csv', index_col=0)
    except FileNotFoundError:
        return

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories = {
        'üìù –¢–ï–ö–°–¢–û–í–´–ï': ['text_length', 'word_count', 'sentence_count', 'avg_sentence_length',
                        'avg_word_length', 'lexical_diversity', 'long_word_ratio', 'text_complexity'],
        'üéØ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï': ['semantic_similarity', 'keyword_overlap', 'tfidf_similarity', 'response_relevance'],
        'üìö –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–ò–ï': ['grammar_error_count', 'grammar_error_ratio', 'has_punctuation',
                             'sentence_completeness', 'proper_capitalization'],
        'üí¨ –î–ò–°–ö–£–†–°': ['has_greeting', 'has_questions', 'has_description', 'has_connectors',
                      'has_emotional_words', 'coherence_score'],
        '‚ùì –¢–ò–ü–´ –í–û–ü–†–û–°–û–í': ['dialog_initiation', 'response_adequacy', 'information_seeking',
                            'descriptive_detail', 'answer_length_sufficiency', 'question_type'],
        '‚≠ê –ö–ê–ß–ï–°–¢–í–û': ['composite_quality_score', 'social_appropriateness', 'interaction_quality']
    }

    print(f"\nüìÇ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
    print("=" * 50)

    numeric_features = features_df.select_dtypes(include=[np.number])

    for category, features in categories.items():
        available_features = [f for f in features if f in numeric_features.columns]
        if available_features:
            print(f"\n{category} ({len(available_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
            for feature in available_features:
                mean_val = numeric_features[feature].mean()
                std_val = numeric_features[feature].std()
                print(f"   ‚Ä¢ {feature:25} {mean_val:6.3f} ¬± {std_val:5.3f}")


if __name__ == "__main__":
    analyze_extracted_features()
    check_feature_correlations_with_target()
    analyze_feature_categories()

    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±—É–¥—É—Ç –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —ç—Ç–æ–º –∞–Ω–∞–ª–∏–∑–µ")